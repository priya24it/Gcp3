import pandas as pd
import pyodbc
import os
import glob
import logging

# Configure logging
log_file = "mismatch_log.txt"
logging.basicConfig(filename=log_file, level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Database connection setup (Replace with actual details)
try:
    conn = pyodbc.connect(
        "DRIVER={SQL Server};"
        "SERVER=your_server;"
        "DATABASE=your_database;"
        "UID=your_username;"
        "PWD=your_password"
    )
    logging.info("Database connection successful.")
except Exception as e:
    logging.error(f"Database connection failed: {str(e)}")
    exit()

# Folder containing CSV files
csv_folder = "path_to_csv_folder"  # Replace with actual folder path

# Get all CSV files in the folder
csv_files = glob.glob(os.path.join(csv_folder, "*.csv"))

# Process each CSV file
for csv_file in csv_files:
    try:
        # Extract table name from file name
        table_name = os.path.basename(csv_file).replace(".csv", "")

        logging.info(f"\nProcessing file: {csv_file} (Table: {table_name})")

        # Fetch unique keys dynamically
        query_unique_keys = f"""
        SELECT COLUMN_NAME 
        FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE 
        WHERE TABLE_NAME = '{table_name}'
        """
        unique_keys_df = pd.read_sql(query_unique_keys, conn)
        unique_keys = unique_keys_df["COLUMN_NAME"].tolist()

        if not unique_keys:
            raise ValueError(f"No unique keys found for table {table_name}")

        # Load CSV Data
        dfcsv = pd.read_csv(csv_file)

        # Fetch SQL Data
        query_data = f"SELECT * FROM {table_name}"
        dfsql = pd.read_sql(query_data, conn)

        # Identify common columns (excluding unique keys)
        common_columns = list(set(dfcsv.columns) & set(dfsql.columns) - set(unique_keys))

        # Perform inner join on unique key columns
        merged_df = pd.merge(dfcsv, dfsql, on=unique_keys, how="inner", suffixes=("_csv", "_sql"))

        # Handle null values by replacing them with an empty string for comparison
        merged_df.fillna("", inplace=True)

        # Convert data types to string for comparison
        for col in common_columns:
            merged_df[f"{col}_csv"] = merged_df[f"{col}_csv"].astype(str)
            merged_df[f"{col}_sql"] = merged_df[f"{col}_sql"].astype(str)

        # Identify mismatches
        mismatch_count = 0
        for col in common_columns:
            mask = merged_df[f"{col}_csv"] != merged_df[f"{col}_sql"]
            mismatch_rows = merged_df.loc[mask, unique_keys].copy()
            mismatch_rows["table_name"] = table_name
            mismatch_rows["column_name"] = col
            mismatch_rows["csv_value"] = merged_df.loc[mask, f"{col}_csv"]
            mismatch_rows["sql_value"] = merged_df.loc[mask, f"{col}_sql"]

            # Log mismatch records (only top 5 mismatches per column for brevity)
            for _, row in mismatch_rows.head(5).iterrows():
                log_message = (f"Mismatch in Table: {table_name}, Column: {col}, "
                               f"Unique Key: {row[unique_keys].to_dict()}, "
                               f"CSV Value: {row['csv_value']}, SQL Value: {row['sql_value']}")
                logging.warning(log_message)
                mismatch_count += 1

        if mismatch_count > 0:
            logging.info(f"{mismatch_count} mismatches found in table {table_name}")

    except Exception as e:
        logging.error(f"Error processing {csv_file}: {str(e)}")

# Close database connection in the finally block
finally:
    try:
        conn.close()
        logging.info("Database connection closed.")
    except Exception as e:
        logging.error(f"Error closing the connection: {str(e)}")

# Print completion message
print(f"Processing complete. Check '{log_file}' for mismatch details.")
